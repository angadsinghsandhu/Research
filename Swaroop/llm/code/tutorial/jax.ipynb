{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX tutorial\n",
    "\n",
    "## Introduction to JAX\n",
    "   - Overview of JAX\n",
    "   - Comparison with NumPy and other libraries like TensorFlow and PyTorch\n",
    "   - Setting up the environment\n",
    "\n",
    "---\n",
    "\n",
    "### Overview of JAX\n",
    "\n",
    "JAX is a high-performance numerical computation library designed for machine learning research and development. It builds on the foundations of NumPy to offer advanced features such as automatic differentiation and the ability to run on GPU and TPU hardware. JAX supports just-in-time compilation and can efficiently handle large-scale, complex numerical computations which are often required in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  4.  9. 16.]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "# Using JAX to perform a simple array operation\n",
    "x = jnp.array([1.0, 2.0, 3.0, 4.0])\n",
    "y = jnp.power(x, 2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with NumPy and Other Libraries Like TensorFlow and PyTorch\n",
    "\n",
    "**NumPy**: JAX extends NumPy by adding automatic differentiation and the option to run on accelerators like GPUs and TPUs. While it mimics the NumPy API, making it familiar and easy to pick up, JAX introduces functional programming paradigms and immutability of arrays.\n",
    "\n",
    "**TensorFlow and PyTorch**:\n",
    "- **TensorFlow**: Like TensorFlow, JAX offers auto-differentiation and GPU/TPU support. However, TensorFlow is designed with static computational graphs, whereas JAX promotes a more dynamic and Pythonic approach, using JIT compilation to optimize computations on the fly.\n",
    "- **PyTorch**: PyTorch is known for its dynamic computation graphs and user-friendly interface. JAX similarly allows for dynamic graphs but with a focus on pure functions and transformations (like grad and jit). JAXâ€™s functional approach can lead to more predictable and optimized code, especially in research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts in JAX\n",
    "   - `jax.numpy` and `jax.random`: Basic operations\n",
    "   - Understanding `jax.jit` for Just-In-Time compilation\n",
    "   - Using `jax.grad` for automatic differentiation\n",
    "   - `jax.vmap` for vectorization\n",
    "   - `jax.pmap` for parallelization across devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of jnp operations: [ 1.3817732   0.49315056 -0.8488725 ]\n",
      "Random matrix:\n",
      " [[-0.3721109   0.26423115 -0.18252768]\n",
      " [-0.7368197   0.44973662 -0.1521442 ]\n",
      " [-0.67135346 -0.5908641   0.73168886]]\n"
     ]
    }
   ],
   "source": [
    "# `jax.numpy` and `jax.random`: Basic Operations\n",
    "# JAX provides `jax.numpy` (often imported as `jnp`), a GPU- and TPU-compatible version of NumPy. It's designed to be used interchangeably with NumPy functions but with the added benefit of JAX's features like auto-differentiation and JIT compilation.\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Basic array operations using jax.numpy\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "y = jnp.sin(x) + jnp.cos(x)\n",
    "print(\"Result of jnp operations:\", y)\n",
    "\n",
    "# `jax.random` is used for random number generation, which is slightly different due to the need for explicit control over the random state:\n",
    "from jax import random\n",
    "\n",
    "# Generating a random matrix\n",
    "key = random.PRNGKey(0)\n",
    "matrix = random.normal(key, (3, 3))\n",
    "print(\"Random matrix:\\n\", matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JIT-compiled sigmoid results: [4.53978719e-05 5.55606748e-05 6.79983132e-05 8.32199730e-05\n",
      " 1.01848804e-04 1.24647180e-04 1.52547960e-04 1.86692982e-04\n",
      " 2.28478792e-04 2.79614789e-04 3.42191313e-04 4.18766722e-04\n",
      " 5.12469152e-04 6.27125031e-04 7.67413818e-04 9.39054938e-04\n",
      " 1.14904228e-03 1.40591967e-03 1.72012544e-03 2.10440415e-03\n",
      " 2.57431110e-03 3.14881280e-03 3.85103165e-03 4.70911246e-03\n",
      " 5.75728714e-03 7.03711621e-03 8.59898701e-03 1.05038397e-02\n",
      " 1.28252096e-02 1.56514868e-02 1.90885402e-02 2.32625399e-02\n",
      " 2.83228736e-02 3.44451889e-02 4.18339297e-02 5.07243499e-02\n",
      " 6.13831095e-02 7.41067529e-02 8.92170370e-02 1.07052118e-01\n",
      " 1.27951682e-01 1.52235821e-01 1.80176586e-01 2.11963370e-01\n",
      " 2.47663721e-01 2.87185848e-01 3.30246359e-01 3.76354516e-01\n",
      " 4.24816966e-01 4.74768937e-01 5.25231123e-01 5.75183094e-01\n",
      " 6.23645484e-01 6.69753551e-01 7.12814093e-01 7.52336144e-01\n",
      " 7.88036704e-01 8.19823325e-01 8.47764194e-01 8.72048259e-01\n",
      " 8.92947793e-01 9.10782874e-01 9.25893307e-01 9.38616931e-01\n",
      " 9.49275613e-01 9.58166063e-01 9.65554833e-01 9.71677125e-01\n",
      " 9.76737440e-01 9.80911493e-01 9.84348476e-01 9.87174809e-01\n",
      " 9.89496171e-01 9.91401017e-01 9.92962897e-01 9.94242728e-01\n",
      " 9.95290875e-01 9.96148944e-01 9.96851146e-01 9.97425616e-01\n",
      " 9.97895598e-01 9.98279929e-01 9.98594105e-01 9.98850942e-01\n",
      " 9.99060929e-01 9.99232650e-01 9.99372900e-01 9.99487519e-01\n",
      " 9.99581277e-01 9.99657869e-01 9.99720395e-01 9.99771535e-01\n",
      " 9.99813378e-01 9.99847412e-01 9.99875307e-01 9.99898195e-01\n",
      " 9.99916792e-01 9.99932051e-01 9.99944448e-01 9.99954581e-01]\n"
     ]
    }
   ],
   "source": [
    "### Understanding `jax.jit` for Just-In-Time Compilation\n",
    "# The `jax.jit` function is used to compile functions to run more efficiently on GPU/TPU. It's particularly useful for functions with operations that JAX can optimize through XLA (Accelerated Linear Algebra).\n",
    "from jax import jit\n",
    "\n",
    "@jit\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + jnp.exp(-x))\n",
    "\n",
    "# JIT-compiled sigmoid function\n",
    "x = jnp.linspace(-10, 10, 100)\n",
    "y = sigmoid(x)\n",
    "print(\"JIT-compiled sigmoid results:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient at w = 10: 10.0\n"
     ]
    }
   ],
   "source": [
    "# Using `jax.grad` for Automatic Differentiation\n",
    "# JAX's `jax.grad` is used to compute the gradient of a function. This is essential for tasks like optimizing training in machine learning.\n",
    "from jax import grad\n",
    "\n",
    "def loss_fn(w):\n",
    "    \"\"\" A simple loss function: (w - 5)^2 \"\"\"\n",
    "    return (w - 5) ** 2\n",
    "\n",
    "# Compute the gradient of the loss function\n",
    "grad_loss = grad(loss_fn)\n",
    "print(\"Gradient at w = 10:\", grad_loss(10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized squaring results: [ 1  4  9 16 25]\n"
     ]
    }
   ],
   "source": [
    "# `jax.vmap` for Vectorization\n",
    "# `jax.vmap` automatically vectorizes a function, enabling it to efficiently operate on batches of inputs without manual batching.\n",
    "from jax import vmap\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "# Automatically vectorize the 'square' function\n",
    "x = jnp.array([1, 2, 3, 4, 5])\n",
    "squared = vmap(square)(x)\n",
    "print(\"Vectorized squaring results:\", squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "compiling computation that requires 2 logical devices, but only 1 XLA devices are available (num_replicas=2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Assuming multiple devices are available, this will parallelize the operation\u001b[39;00m\n\u001b[0;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m6\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m---> 11\u001b[0m cubed \u001b[38;5;241m=\u001b[39m \u001b[43mcube\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# if this gives an error it mean you only have 1 single device\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParallel cube results:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cubed)\n",
      "    \u001b[1;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\jax\\_src\\interpreters\\pxla.py:928\u001b[0m, in \u001b[0;36mUnloadedPmapExecutable.from_hlo\u001b[1;34m(hlo, pci, replicas, shards, tuple_args, unordered_effects, ordered_effects, host_callbacks, keepalive, jaxpr_debug_info, compiler_options)\u001b[0m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shards\u001b[38;5;241m.\u001b[39mnum_global_shards \u001b[38;5;241m>\u001b[39m xb\u001b[38;5;241m.\u001b[39mdevice_count(pci\u001b[38;5;241m.\u001b[39mbackend):\n\u001b[0;32m    926\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiling computation that requires \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m logical devices, but only \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m XLA \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    927\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevices are available (num_replicas=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 928\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(shards\u001b[38;5;241m.\u001b[39mnum_global_shards,\n\u001b[0;32m    929\u001b[0m                               xb\u001b[38;5;241m.\u001b[39mdevice_count(pci\u001b[38;5;241m.\u001b[39mbackend),\n\u001b[0;32m    930\u001b[0m                               replicas\u001b[38;5;241m.\u001b[39mnum_global_replicas))\n\u001b[0;32m    931\u001b[0m \u001b[38;5;66;03m# On a single host, we simply grab the first N devices from jax.devices().\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;66;03m# In the single host case, we want the default device order of pmap to\u001b[39;00m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;66;03m# match jax.devices().\u001b[39;00m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;66;03m# On multiple hosts, we create a default device assignment that ensures\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;66;03m# each host is responsible for a contiguous set of replicas.\u001b[39;00m\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shards\u001b[38;5;241m.\u001b[39mnum_global_shards \u001b[38;5;241m>\u001b[39m shards\u001b[38;5;241m.\u001b[39mnum_local_shards:\n\u001b[0;32m    937\u001b[0m   \u001b[38;5;66;03m# TODO(skye): use a locality-aware assignment that satisfies the above\u001b[39;00m\n\u001b[0;32m    938\u001b[0m   \u001b[38;5;66;03m# constraint.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: compiling computation that requires 2 logical devices, but only 1 XLA devices are available (num_replicas=2)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# `jax.pmap` for Parallelization Across Devices\n",
    "# `jax.pmap` (parallel map) is used for parallel computation across multiple devices like GPUs and TPUs, making it possible to scale computations efficiently.\n",
    "from jax import pmap\n",
    "\n",
    "@pmap\n",
    "def cube(x):\n",
    "    return x ** 3\n",
    "\n",
    "# Assuming multiple devices are available, this will parallelize the operation\n",
    "x = jnp.arange(6).reshape((2, 3))\n",
    "cubed = cube(x) # if this gives an error it mean you only have 1 single device\n",
    "print(\"Parallel cube results:\", cubed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra in JAX\n",
    "   - Implementing basic linear layers\n",
    "   - Activation functions\n",
    "\n",
    "Linear algebra is foundational to many operations in machine learning and data science. JAX provides efficient and flexible tools for performing these operations on modern hardware. This section will cover implementing basic linear layers and activation functions in JAX.\n",
    "\n",
    "### Implementing Basic Linear Layers\n",
    "\n",
    "A linear layer in a neural network performs a linear transformation \\( y = Wx + b \\), where \\( W \\) is the weight matrix, \\( x \\) is the input vector or matrix, and \\( b \\) is the bias vector. Hereâ€™s how you can implement a simple linear layer in JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of linear layer: [ 2.625999   1.5546751 -1.608583 ]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "def init_linear_layer(input_dim, output_dim, key):\n",
    "    \"\"\" Initialize weights and biases for a linear layer. \"\"\"\n",
    "    W_key, b_key = random.split(key)\n",
    "    W = random.normal(W_key, (output_dim, input_dim)) * (2 / input_dim)**0.5\n",
    "    b = random.normal(b_key, (output_dim,)) * (2 / input_dim)**0.5\n",
    "    return W, b\n",
    "\n",
    "def linear_layer(x, W, b):\n",
    "    \"\"\" Apply a linear transformation to the input data x. \"\"\"\n",
    "    return jnp.dot(W, x) + b\n",
    "\n",
    "# Example usage\n",
    "key = random.PRNGKey(0)\n",
    "input_dim, output_dim = 4, 3\n",
    "W, b = init_linear_layer(input_dim, output_dim, key)\n",
    "x = jnp.array([1, 2, 3, 4])\n",
    "y = linear_layer(x, W, b)\n",
    "print(\"Output of linear layer:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing and applying these functions in JAX takes advantage of JIT compilation and auto-vectorization, allowing them to run very efficiently on both CPUs and GPUs.\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Activation functions are crucial in neural networks as they introduce non-linear properties to the system, allowing for learning complex patterns. Common activation functions include ReLU, sigmoid, and tanh. Hereâ€™s how to implement these in JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU: [0. 0. 0. 0. 1. 2. 3.]\n",
      "Sigmoid: [0.04742587 0.11920292 0.26894143 0.5        0.7310586  0.880797\n",
      " 0.95257413]\n",
      "Tanh: [-0.9950547 -0.9640276 -0.7615942  0.         0.7615942  0.9640276\n",
      "  0.9950547]\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    \"\"\" Rectified Linear Unit activation function \"\"\"\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid activation function \"\"\"\n",
    "    return 1 / (1 + jnp.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\" Hyperbolic Tangent activation function \"\"\"\n",
    "    return jnp.tanh(x)\n",
    "\n",
    "# Applying activation functions\n",
    "x = jnp.linspace(-3, 3, 7)\n",
    "print(\"ReLU:\", relu(x))\n",
    "print(\"Sigmoid:\", sigmoid(x))\n",
    "print(\"Tanh:\", tanh(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Networks from Scratch\n",
    "   - Implementing common layers manually (Convolutional, Recurrent, etc.)\n",
    "   - Creating loss functions\n",
    "   - Training loops\n",
    "\n",
    "---\n",
    "\n",
    "## Building Neural Networks from Scratch\n",
    "\n",
    "Building neural networks from scratch can deepen your understanding of their inner workings. This section will demonstrate how to implement common types of layers, define loss functions, and construct training loops using JAX.\n",
    "\n",
    "### Implementing Common Layers Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "conv_general_dilated lhs feature dimension size divided by feature_group_count must equal the rhs input feature dimension size, but 5 // 1 != 3.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m input_feature_map \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Shape: (batch_size, height, width, channels)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m kernel \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Shape: (height, width, in_channels, out_channels)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m output_feature_map \u001b[38;5;241m=\u001b[39m \u001b[43mconv_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_feature_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSAME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput of convolutional layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, output_feature_map)\n",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m, in \u001b[0;36mconv_layer\u001b[1;34m(x, W, stride, padding)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconv_layer\u001b[39m(x, W, stride, padding):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Apply a convolutional layer manually \"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_general_dilated\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# input\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# kernel\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_strides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# stride\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# padding\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\jax\\_src\\lax\\convolution.py:161\u001b[0m, in \u001b[0;36mconv_general_dilated\u001b[1;34m(lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation, dimension_numbers, feature_group_count, batch_group_count, precision, preferred_element_type)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    155\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadding argument to conv_general_dilated should be a string or a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence of (low, high) pairs, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpadding\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    158\u001b[0m preferred_element_type \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m preferred_element_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(np\u001b[38;5;241m.\u001b[39mdtype(preferred_element_type)))\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv_general_dilated_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_strides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwindow_strides\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlhs_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlhs_dilation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs_dilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrhs_dilation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdnums\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_group_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_group_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_group_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_group_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanonicalize_precision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\jax\\_src\\core.py:422\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[1;34m(self, *args, **params)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    420\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    421\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[1;32m--> 422\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\jax\\_src\\core.py:425\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[1;34m(self, trace, args, params)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m--> 425\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\jax\\_src\\core.py:913\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[1;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[1;32m--> 913\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\jax\\_src\\dispatch.py:87\u001b[0m, in \u001b[0;36mapply_primitive\u001b[1;34m(prim, *args, **params)\u001b[0m\n\u001b[0;32m     85\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m   outs \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
      "    \u001b[1;31m[... skipping hidden 18 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\jax\\_src\\lax\\convolution.py:374\u001b[0m, in \u001b[0;36m_conv_general_dilated_shape_rule\u001b[1;34m(lhs, rhs, window_strides, padding, lhs_dilation, rhs_dilation, dimension_numbers, feature_group_count, batch_group_count, **unused_kwargs)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core\u001b[38;5;241m.\u001b[39mdefinitely_equal(quot, rhs\u001b[38;5;241m.\u001b[39mshape[dimension_numbers\u001b[38;5;241m.\u001b[39mrhs_spec[\u001b[38;5;241m1\u001b[39m]]):\n\u001b[0;32m    371\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv_general_dilated lhs feature dimension size divided by \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_group_count must equal the rhs input feature dimension \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize, but \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m // \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 374\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(lhs_feature_count, feature_group_count,\n\u001b[0;32m    375\u001b[0m                               rhs\u001b[38;5;241m.\u001b[39mshape[dimension_numbers\u001b[38;5;241m.\u001b[39mrhs_spec[\u001b[38;5;241m1\u001b[39m]]))\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rhs\u001b[38;5;241m.\u001b[39mshape[dimension_numbers\u001b[38;5;241m.\u001b[39mrhs_spec[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m%\u001b[39m feature_group_count:\n\u001b[0;32m    377\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv_general_dilated rhs output feature dimension size must be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    378\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple of feature_group_count, but \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not a multiple of \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: conv_general_dilated lhs feature dimension size divided by feature_group_count must equal the rhs input feature dimension size, but 5 // 1 != 3."
     ]
    }
   ],
   "source": [
    "## Convolutional Layers\n",
    "# Convolutional layers are fundamental in processing spatial data such as images. Here's how to implement a basic convolutional layer in JAX:\n",
    "from jax import lax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def conv_layer(x, W, stride, padding):\n",
    "    \"\"\" Apply a convolutional layer manually \"\"\"\n",
    "    return lax.conv_general_dilated(\n",
    "        x,  # input\n",
    "        W,  # kernel\n",
    "        window_strides=(stride, stride),  # stride\n",
    "        padding=padding  # padding\n",
    "    )\n",
    "\n",
    "# Example usage for convolutional layer\n",
    "input_feature_map = jnp.ones((1, 5, 5, 1))  # Shape: (batch_size, height, width, channels)\n",
    "kernel = jnp.ones((3, 3, 1, 1))  # Shape: (height, width, in_channels, out_channels)\n",
    "output_feature_map = conv_layer(input_feature_map, kernel, stride=1, padding='SAME')\n",
    "print(\"Output of convolutional layer:\\n\", output_feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next hidden state: [-0.9753993  -0.49872476 -0.8934176  -0.40455788 -0.7315557  -0.90595424\n",
      "  0.869794    0.997634   -0.37609205 -0.99607754]\n"
     ]
    }
   ],
   "source": [
    "## Recurrent Layers\n",
    "# Recurrent layers are essential for processing sequential data. Implementing a simple recurrent neural network (RNN) layer manually can be done as follows:\n",
    "def rnn_cell(h_prev, x_t, W_h, W_x, b):\n",
    "    \"\"\" A simple RNN cell \"\"\"\n",
    "    return jnp.tanh(jnp.dot(W_h, h_prev) + jnp.dot(W_x, x_t) + b)\n",
    "\n",
    "# Example usage for RNN cell\n",
    "h_prev = jnp.zeros(10)  # previous hidden state\n",
    "x_t = jnp.ones(5)  # current input\n",
    "W_h = random.normal(key, (10, 10))  # weights for previous hidden state\n",
    "W_x = random.normal(key, (10, 5))  # weights for current input\n",
    "b = jnp.zeros(10)  # bias\n",
    "h_next = rnn_cell(h_prev, x_t, W_h, W_x, b)\n",
    "print(\"Next hidden state:\", h_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.16666667\n"
     ]
    }
   ],
   "source": [
    "## Creating Loss Functions\n",
    "# Loss functions evaluate how well your model performs. Here is an example of a common loss function used in regression tasks:\n",
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\" Mean Squared Error loss function \"\"\"\n",
    "    return jnp.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "# Example usage of MSE loss\n",
    "y_pred = jnp.array([0.5, 2.0, 2.5])\n",
    "y_true = jnp.array([0, 2, 3])\n",
    "loss = mse_loss(y_pred, y_true)\n",
    "print(\"MSE Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated parameters: [Array([ 1.3704304, -0.1295694], dtype=float32), Array([0.31756377], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "## Training Loops\n",
    "# Training loops involve repeatedly adjusting model parameters to minimize the loss. Here's a basic example of a training loop using JAX:\n",
    "from jax import grad, jit\n",
    "\n",
    "@jit\n",
    "def update_params(params, grads, learning_rate):\n",
    "    \"\"\" Update parameters using gradient descent \"\"\"\n",
    "    return [p - learning_rate * g for p, g in zip(params, grads)]\n",
    "\n",
    "# Dummy data and parameters\n",
    "params = [jnp.array([1.0, -0.5]), jnp.array([0.0])]  # example parameters\n",
    "data = jnp.array([[1.0, 2.0], [2.0, 3.0]])  # example data\n",
    "targets = jnp.array([1.5, 2.5])  # example targets\n",
    "\n",
    "# Simulate a training loop\n",
    "learning_rate = 0.01\n",
    "for epoch in range(100):\n",
    "    y_pred = jnp.dot(data, params[0]) + params[1]\n",
    "    loss = mse_loss(y_pred, targets)\n",
    "    grads = grad(mse_loss)(y_pred, targets)\n",
    "    params = update_params(params, grads, learning_rate)\n",
    "\n",
    "print(\"Updated parameters:\", params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Neural Network Architectures\n",
    "   - Convolutional Neural Networks (CNNs) for image processing\n",
    "   - Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) for sequence processing\n",
    "   - Transformers for NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization and Training Techniques\n",
    "   - Gradient descent and its variants\n",
    "   - Techniques for improving training stability and performance\n",
    "   - Learning rate schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Testing\n",
    "   - Validation and cross-validation\n",
    "   - Overfitting and regularization techniques\n",
    "   - Metrics for performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities and Helpers in JAX\n",
    "   - Using `jax.tree_util` to handle complex data structures\n",
    "   - Debugging tools in JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Frameworks for JAX\n",
    "   - Introduction to Flax and Haiku: Higher-level abstractions for neural networks\n",
    "   - Using Optax for gradient processing and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization and Model Optimization\n",
    "    - Understanding quantization in deep learning\n",
    "    - Implementing quantization in JAX for model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel and Distributed Computing\n",
    "    - Using `jax.pmap` for data parallelism\n",
    "    - Strategies for distributing computation across multiple GPUs or TPUs\n",
    "    - Best practices for parallel and distributed training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topics and Applications\n",
    "    - Mixed precision training for efficiency\n",
    "    - Integrating JAX with other Python libraries\n",
    "    - Real-world applications and case studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. **JAX for Research**\n",
    "    - How to leverage JAX for experimental and cutting-edge research\n",
    "    - Custom gradients and extending JAX with custom operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. **Performance Tuning and Optimization**\n",
    "    - Profiling JAX applications\n",
    "    - Techniques for maximizing performance on GPUs and TPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF THE LINE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
