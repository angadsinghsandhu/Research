{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ESM models from hugging face\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model name for the ESM model you wish to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"facebook/esm2_t48_15B_UR50D\" # 48 layers, 15B parameters, 67 GB\n",
    "# model_name = \"facebook/esm2_t36_3B_UR50D\" # 36 layers, 3B parameters, 18 GB\n",
    "# model_name = \"facebook/esm2_t33_650M_UR50D\" # 33 layers, 650M parameters, 2.5 GB\n",
    "# model_name = \"facebook/esm2_t30_150M_UR50D\" # 30 layers, 150M parameters\n",
    "# model_name = \"facebook/esm2_t12_35M_UR50D\" # 12 layers, 35M parameters\n",
    "model_name = \"facebook/esm2_t6_8M_UR50D\" # 6 layers, 8M parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pretrained Model and Tokenizer from Hugging Face's Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a770762e3ee54627a60e5fd47ea100d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\angad\\.cache\\huggingface\\hub\\models--facebook--esm2_t6_8M_UR50D. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11357b1cdec64434a0cb05a6ef972f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/93.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3187aea257740be8391b2f408af6cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a574dc970140b184b42e42118fddf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b9fb8c03154604b6411b5d33457790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/31.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pretrained model and tokenizer from Hugging Face's transformers library\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 6\n"
     ]
    }
   ],
   "source": [
    "# Get the model's layers\n",
    "layers = model.config.num_hidden_layers\n",
    "\n",
    "# Print the layers\n",
    "print(\"Number of layers:\", layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1:\n",
      "Number of parameters: 1232960\n",
      "Layer type: EsmLayer\n",
      "\n",
      "Layer 2:\n",
      "Number of parameters: 1232960\n",
      "Layer type: EsmLayer\n",
      "\n",
      "Layer 3:\n",
      "Number of parameters: 1232960\n",
      "Layer type: EsmLayer\n",
      "\n",
      "Layer 4:\n",
      "Number of parameters: 1232960\n",
      "Layer type: EsmLayer\n",
      "\n",
      "Layer 5:\n",
      "Number of parameters: 1232960\n",
      "Layer type: EsmLayer\n",
      "\n",
      "Layer 6:\n",
      "Number of parameters: 1232960\n",
      "Layer type: EsmLayer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get information about each layer\n",
    "for i, layer_module in enumerate(model.base_model.encoder.layer):\n",
    "    print(f\"Layer {i + 1}:\")\n",
    "    print(\"Number of parameters:\", sum(p.numel() for p in layer_module.parameters() if p.requires_grad))\n",
    "    print(\"Layer type:\", layer_module.__class__.__name__)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your protein sequences here. Replace '...' with your actual protein sequences.\n",
    "# For demonstration, these are placeholders and should be replaced with real sequences.\n",
    "sequences = [\n",
    "    \"VMHLTPEEKSAVTALWGKVNVDEVGGEALGRLLVVYPWTQRFFESFGDLSTPDAVMGNPKVKAHGKKVLGAFSDGLAHLDNLKGTFATLSELHCDKLHVDPENFRLLGNVLVCVLAHHFGKEFTPPVQAAYQKVVAGVANALAHKYH\",\n",
    "    \"MEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGPDEAPRMPEAAPPVAPAPAAPTPAAPAPAPSWPLSSSVPSQKTYQGSYGFRLGFLHSGTAKSVTCTYSPALNKMFCQLAKTCPVQLWVDSTPPPGTRVRAMAIYKQSQHMTEVVRRCPHGRSCSDG\",\n",
    "    \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLTYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGGVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITLGMDELYK\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize input and run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the protein sequences. The tokenizer converts sequences into a format the model can understand.\n",
    "inputs = tokenizer(sequences, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# We do not need to calculate gradients (useful for inference), hence torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    # Pass the tokenized sequences through the model.\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Model Output ====\n",
      "torch.Size([3, 240, 33])\n",
      "==== Model Output ====\n",
      "Embeddings shape: torch.Size([3, 240, 320])\n"
     ]
    }
   ],
   "source": [
    "print(\"==== Model Output ====\")\n",
    "# output shape : (batch_size, sequence_length, num_labels)\n",
    "print(outputs.logits.shape)\n",
    "print(\"==== Model Output ====\")\n",
    "\n",
    "# Check if 'hidden_states' is part of the output\n",
    "if 'hidden_states' in outputs:\n",
    "    # The hidden states are typically a tuple with each element being the states of a layer\n",
    "    # The last element ([-1]) of this tuple will give you the last layer's hidden states, often used as the embeddings\n",
    "    embeddings = outputs['hidden_states'][-1]\n",
    "    print(\"Embeddings shape:\", embeddings.shape)\n",
    "else:\n",
    "    # If there are no hidden_states, this model might not have been configured to output them\n",
    "    # You might need to check your model configuration or use 'logits' for different purposes\n",
    "    print(\"No hidden states available in the model output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__class__', '__class_getitem__', '__contains__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__or__', '__post_init__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'attentions', 'clear', 'copy', 'fromkeys', 'get', 'hidden_states', 'items', 'keys', 'logits', 'loss', 'move_to_end', 'pop', 'popitem', 'setdefault', 'to_tuple', 'update', 'values']\n"
     ]
    }
   ],
   "source": [
    "print(dir(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 240])\n",
      "torch.Size([3, 240, 1280])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.input_ids.shape)\n",
    "print(outputs.hidden_states[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 238, 1280])\n"
     ]
    }
   ],
   "source": [
    "# Get the output before the last 2 layers\n",
    "output_before_last_2_layers = outputs.hidden_states[-1][:, :-2, :]\n",
    "print(output_before_last_2_layers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method values of MaskedLMOutput object at 0x000002A7192BF940>\n"
     ]
    }
   ],
   "source": [
    "print(outputs.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM model for masked language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"facebook/esm2_t48_15B_UR50D\" # 48 layers, 15B parameters, 67 GB\n",
    "# model_name = \"facebook/esm2_t36_3B_UR50D\" # 36 layers, 3B parameters, 18 GB\n",
    "# model_name = \"facebook/esm2_t33_650M_UR50D\" # 33 layers, 650M parameters, 2.5 GB\n",
    "# model_name = \"facebook/esm2_t30_150M_UR50D\" # 30 layers, 150M parameters\n",
    "# model_name = \"facebook/esm2_t12_35M_UR50D\" # 12 layers, 35M parameters\n",
    "model_name = \"facebook/esm2_t6_8M_UR50D\" # 6 layers, 8M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your protein sequences here. Replace '...' with your actual protein sequences.\n",
    "# For demonstration, these are placeholders and should be replaced with real sequences.\n",
    "sequences = [\n",
    "    \"VMHLTPEEKSAVTALWGKVNVDEVGGEALGRLLVVYPWTQRFFESFGDLSTPDAVMGNPKVKAHGKKVLGAFSDGLAHLDNLKGTFATLSELHCDKLHVDPENFRLLGNVLVCVLAHHFGKEFTPPVQAAYQKVVAGVANALAHKYH\",\n",
    "    \"MEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGPDEAPRMPEAAPPVAPAPAAPTPAAPAPAPSWPLSSSVPSQKTYQGSYGFRLGFLHSGTAKSVTCTYSPALNKMFCQLAKTCPVQLWVDSTPPPGTRVRAMAIYKQSQHMTEVVRRCPHGRSCSDG\",\n",
    "    \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLTYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGGVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITLGMDELYK\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the protein sequences. The tokenizer converts sequences into a format the model can understand.\n",
    "inputs = tokenizer(sequences, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# We do not need to calculate gradients (useful for inference), hence torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    # Pass the tokenized sequences through the model.\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Model Output ====\n",
      "torch.Size([3, 240, 33])\n",
      "==== Model Output ====\n"
     ]
    }
   ],
   "source": [
    "print(\"==== Model Output ====\")\n",
    "# output shape : (batch_size, sequence_length, num_labels)\n",
    "print(outputs.logits.shape)\n",
    "print(\"==== Model Output ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Encoder hyperparamerters\n",
    "model_name = \"facebookresearch/esm\"\n",
    "\n",
    "# select the model\n",
    "# model_type = \"esm2_t48_15B_UR50D\" # 48 layers, 15B parameters, 67 GB\n",
    "# model_type = \"esm2_t36_3B_UR50D\" # 36 layers, 3B parameters, 18 GB\n",
    "# model_type = \"esm2_t33_650M_UR50D\" # 33 layers, 650M parameters, 2.5 GB\n",
    "# model_type = \"esm2_t30_150M_UR50D\" # 30 layers, 150M parameters\n",
    "# model_type = \"esm2_t12_35M_UR50D\" # 12 layers, 35M parameters\n",
    "model_type = \"esm2_t6_8M_UR50D\" # 6 layers, 8M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\angad/.cache\\torch\\hub\\facebookresearch_esm_main\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt\" to C:\\Users\\angad/.cache\\torch\\hub\\checkpoints\\esm2_t6_8M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t6_8M_UR50D-contact-regression.pt\" to C:\\Users\\angad/.cache\\torch\\hub\\checkpoints\\esm2_t6_8M_UR50D-contact-regression.pt\n"
     ]
    }
   ],
   "source": [
    "encoder, alphabet = torch.hub.load(model_name, model_type)\n",
    "batch_converter = alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<esm.data.Alphabet object at 0x0000020162177E10>\n",
      "All attributes and methods: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_tokenize', 'all_special_tokens', 'all_toks', 'append_eos', 'append_toks', 'cls_idx', 'encode', 'eos_idx', 'from_architecture', 'get_batch_converter', 'get_idx', 'get_tok', 'mask_idx', 'padding_idx', 'prepend_bos', 'prepend_toks', 'standard_toks', 'to_dict', 'tok_to_idx', 'tokenize', 'unique_no_split_tokens', 'unk_idx', 'use_msa']\n"
     ]
    }
   ],
   "source": [
    "print(alphabet)\n",
    "\n",
    "# 1. List all attributes and methods using dir()\n",
    "all_attrs_methods = dir(alphabet)\n",
    "print(\"All attributes and methods:\", all_attrs_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_init_submodules', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'alphabet', 'alphabet_size', 'append_eos', 'apply', 'attention_heads', 'bfloat16', 'buffers', 'call_super_init', 'children', 'cls_idx', 'compile', 'contact_head', 'cpu', 'cuda', 'double', 'dump_patches', 'emb_layer_norm_after', 'embed_dim', 'embed_scale', 'embed_tokens', 'eos_idx', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'layers', 'lm_head', 'load_state_dict', 'mask_idx', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_layers', 'padding_idx', 'parameters', 'predict_contacts', 'prepend_bos', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'token_dropout', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
      "320\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(dir(encoder))\n",
    "print(encoder.embed_dim)\n",
    "print(encoder.alphabet_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alphabet.all_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, seqs, inputs = batch_converter([\n",
    "    (\"abc\", \"KRKRTRFTPEQLEILEAIFKQNPYPSREEREELAKELGLSEKQVKVWFQNRRAKERK\"),\n",
    "    (\"bc\", \"KRKRTRFTPEQLEILEAIFKQNPYPSREEREELAKELGLSEKQVKVWFQNRRAKERK\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_out = encoder(inputs, repr_layers=[6], return_contacts=True)\n",
    "emb = esm_out[\"representations\"][6]         # emb shape : [2, 74, 1280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of inputs: torch.Size([2, 59]):\n",
      "shape of encoder output: torch.Size([2, 59, 320])\n"
     ]
    }
   ],
   "source": [
    "print(f\"shape of inputs: {inputs.shape}:\")\n",
    "print(f\"shape of encoder output: {emb.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
