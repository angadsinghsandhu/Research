{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# K-Means Clustering Example with Word2Vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "# importing gensim for creating embeddings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# importing nltk for clutering and handling data\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "\n",
    "# importing nltk for clutering and handling metrics\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "source": [
    "### *Getting Dummy Data (to be later replaced by direct embeddings from diferent notebooks)*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['this', 'is', 'the', 'good', 'machine', 'learning', 'book'],\n",
    "            ['this', 'is',  'another', 'book'],\n",
    "            ['one', 'more', 'book'],\n",
    "            ['this', 'is', 'the', 'new', 'post'],\n",
    "            ['this', 'is', 'about', 'machine', 'learning', 'post'],  \n",
    "            ['and', 'this', 'is', 'the', 'last', 'post']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "IPyKernel not installed into interpreter Python 3.8.8 64-bit ('tf': conda):C:\\Users\\angad\\anaconda3\\envs\\tf\\python.exe",
     "traceback": [
      "Error: IPyKernel not installed into interpreter Python 3.8.8 64-bit ('tf': conda):C:\\Users\\angad\\anaconda3\\envs\\tf\\python.exe",
      "at v.installMissingDependencies (c:\\Users\\angad\\.vscode\\extensions\\ms-toolsai.jupyter-2021.6.832593372\\out\\client\\extension.js:90:244799)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)"
     ]
    }
   ],
   "source": [
    "# creating Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "source": [
    "### *Testing the created Model*\n",
    "\n",
    "Now we have model with words embedded. We can query model for similar words like below or ask to represent words as vectors."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model.similarity('this', 'is'))\n",
    "print (model.similarity('post', 'book'))\n",
    "#output -0.0198180344218\n",
    "#output -0.079446731287\n",
    "\n",
    "print (model.most_similar(positive=['machine'], negative=[], topn=2))\n",
    "#output: [('new', 0.24608060717582703), ('is', 0.06899910420179367)]\n",
    "\n",
    "print (model['the'])\n",
    "#output [-0.00217354 -0.00237131  0.00296396 ...,  0.00138597  0.00291924  0.00409528]"
   ]
  },
  {
   "source": [
    "### *Getting Vocabulary*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = model.wv.index_to_word\n",
    "print (list(vocab))\n",
    "print (len(list(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation of word embedding to be used\n",
    "X = model[vocab]"
   ]
  },
  {
   "source": [
    "*Now we will feed word embeddings into `clustering algorithm` such as `k-Means` which is one of the most popular `unsupervised learning algorithms` for finding `interesting segments` in the data. It can be used for separating customers into groups, combining documents into topics and for many other applications.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### *Clustering Using NLTK*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS=3\n",
    "\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "\n",
    "print (assigned_clusters)\n",
    "# output: [0, 2, 1, 2, 2, 1, 2, 2, 0, 1, 0, 1, 2, 1, 2]"
   ]
  },
  {
   "source": [
    "<ins>__nltk.cluster.util.cosine_distance(u, v)__</ins>  \n",
    "Returns 1 minus the cosine of the angle between vectors v and u. This is equal to `1 – (u.v / |u||v|)`.\n",
    "\n",
    "<ins>__nltk.cluster.util.euclidean_distance(u, v)__ </ins>  \n",
    "Returns the euclidean distance between vectors u and v. This is equivalent to the length of the vector `(u – v)`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have got the cluster results \n",
    "# We can associate each word with the cluster that it got assigned to...\n",
    "words = list(model.vocab)\n",
    "for i, word in enumerate(words):  \n",
    "    print (word + \":\" + str(assigned_clusters[i]))"
   ]
  },
  {
   "source": [
    "### *Clustering using SKLEARN*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "kmeans.fit(X)\n",
    " \n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    " \n",
    "print (\"Cluster id labels for inputted data\")\n",
    "print (labels)\n",
    "print (\"Centroids data\")\n",
    "print (centroids)"
   ]
  },
  {
   "source": [
    "### *Getting some useful metrics to estimate clustering performance.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\")\n",
    "print (kmeans.score(X))\n",
    " \n",
    "silhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')\n",
    " \n",
    "print (\"Silhouette_score: \")\n",
    "print (silhouette_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster id labels for inputted data\n",
    "# [0 1 1 ..., 1 2 2]\n",
    "# Centroids data\n",
    "# [[ -3.82586889e-04   1.39791325e-03  -2.13839358e-03 ...,  -8.68172920e-04\n",
    "#    -1.23599875e-03   1.80053393e-03]\n",
    "#  [ -3.11774168e-04  -1.63297475e-03   1.76715955e-03 ...,  -1.43826099e-03\n",
    "#     1.22940990e-03   1.06353679e-03]\n",
    "#  [  1.91571176e-04   6.40696089e-04   1.38173658e-03 ...,  -3.26442620e-03\n",
    "#    -1.08828480e-03  -9.43636987e-05]]\n",
    " \n",
    "# Score (Opposite of the value of X on the \n",
    "# K-means objective which is Sum of distances \n",
    "# of samples to their closest cluster center):\n",
    "# -0.00894730946094\n",
    "\n",
    "# Silhouette_score: \n",
    "# 0.0427737"
   ]
  }
 ]
}