{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Here we take a look at the different ways in which we can combine `LDA` and `Word2Vec`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Method-1 : Taxonomy Learning (Hypernym Discovery)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![Hypernym Disovery](http://1.bp.blogspot.com/_4Jat6DCd6RE/TTDmXibqlrI/AAAAAAAAAAU/P9nnhyPXLyA/s1600/hypernym%2526hyponym.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### *Advantages*\n",
    "\n",
    "- [x]  Performs Hierarchical Classification (hence, accounts for semantic data)\n",
    "- [x]  Hypernym provides us with the supposed topic name\n",
    "- [x]  Has been worked on for the last 20 years\n",
    "<br>\n",
    "### *Disadvantages*\n",
    "- [ ]  Compplicated Implementation\n",
    "- [ ]  Recent advancemnts in 2016, hence no proper Libarary implimenattions\n",
    "- [ ]  Limited implementations usng Word Embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Method-2 : Embedded Topic Modeling (__ETM__) [topic modeling first, then embeddings ]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. \n",
    "\n",
    "To this end, we develop the embedded topic model __(**etm**)__, a generative model of documents that `marries traditional topic models with word embeddings`. \n",
    "\n",
    "More specifically, the etm models each word with a `categorical distribution` whose natural parameter is the inner product between the wordâ€™s embedding and an embedding of its assigned topic. \n",
    "\n",
    "It `outperforms` existing document models, such as latent Dirichlet allocation, in terms of both topic quality and predictive performance."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![LDA vs ETM](https://mitp.silverchair-cdn.com/mitp/content_public/journal/tacl/8/10.1162_tacl_a_00325/5/m_00325f01.png?Expires=1626904728&Signature=icDdBaJJW2~4~ZZ3En04x11WpnkUwp9t988qoKB4EKmzDhvcPnYCbN3lr~eTpkA2epMASwZ7pAOHePBvykXSyH48AH2MQ0DDKwpNd~98BM~Az9uPtLEBEb5frgG-yM6prXFUXewEFA~asKIgOQqebyE4CvD29iCnzHdBfqcLO9ZFsxwfOmkQSc1N3kkdjyBcYCYtVsqi3mjJ4O4LVjla~4RU0e7zcj1Icy4yrRXMmcFIwtBKJcDZ2eYd7KS6jrrsP7FU1A8EOxkOAQViSKtGRr4slcd9A9v0sj6v~LsBZ0NwLHVlpoIFbgbf1uTLJYxSklYo9L3Sp3flmrbZ1Z~FJw__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### *Advantages*\n",
    "\n",
    "- [x]  Easy Implementation\n",
    "- [x]  Uses Both, Embeddings and LDA\n",
    "- [x]  Good For Lr=arge Corpuses\n",
    "<br>\n",
    "### *Disadvantages*\n",
    "- [ ]  Obsure Python Library, not really tested\n",
    "- [ ]  No proper Visualization available"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### *Implementation*\n",
    "\n",
    "For actually implementing ETM, we can use a pre-built python library avalable through `Pypi`\n",
    "This Library is called *`embedded-topic-model 1.0.1`*,\n",
    "\n",
    "Using this library we pwerform preprocessing on our corpus and then create a `embedding` object that is passed into the `ETM` object.\n",
    "\n",
    "We can also use [this](https://github.com/adjidieng/ETM) python module avalable on GitHub.\n",
    "\n",
    "For moe information on implementation of `ETM`s refer to [this](https://paperswithcode.com/paper/topic-modeling-in-embedding-spaces) link."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Method-3 : lda2vec [first embedding, then LDA]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Inspired by Latent Dirichlet Allocation (LDA), the word2vec model is expanded to simultaneously learn `word`, `document` and `topic` vectors.\n",
    "\n",
    "Lda2vec is obtained by modifying the `skip-gram word2vec variant`. In the original skip-gram method, the model is trained to predict context words based on a `pivot word`. In lda2vec, the `pivot word vector` and a `document vecto`r are added to obtain a `context vector`. This context vector is then `used to predict context words`.\n",
    "\n",
    "In the next section, you will see how these document vectors are constructed and how they can be used similarly as document vectors in LDA."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Word-2-Vec : <br>\n",
    "![Word2Vec](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1507675358/anim00_waknxv.gif)\n",
    "<br><br>LDA-2-VEC : <br>\n",
    "![Lda2Vec](https://miro.medium.com/max/774/0*10nhh6L3hnpxeyeT.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### *Advantages*\n",
    "\n",
    "- [x]  Easy Implementation\n",
    "- [x]  Upgrade on Highly used Word2Vec\n",
    "- [x]  Visualizations Available\n",
    "<br>\n",
    "### *Disadvantages*\n",
    "- [ ]  Less Number of metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### *Implementation*\n",
    "\n",
    "For actually implementing lda2vec, we can use the following pre-built NLP python library avalable\n",
    "such as,\n",
    "\n",
    "\n",
    "CNTK, Theano, etc. ALso, a Tensorflow implementation was also made publicly available. we can also use the `lda2vec` python module at [this](https://github.com/cemoody/lda2vec/blob/master/docs/index.rst) GitHub Link.\n",
    "\n",
    "We can also use [this](https://medium.com/analytics-vidhya/topic-modelling-using-word-embeddings-and-latent-dirichlet-allocation-3494778307bc) method using k-means for clustering separately.\n",
    "\n",
    "*NOTE : *As training lda2vec can be computationally intensive, GPU support is recommended for larger corpora"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}